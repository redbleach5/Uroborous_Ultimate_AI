llm:
  default_provider: ollama
  providers:
    openai:
      enabled: true
      default_model: gpt-4-turbo-preview
      base_url: https://api.openai.com/v1
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
      auto_discover: false
      distributed_mode: false
    anthropic:
      enabled: true
      default_model: claude-3-opus-20240229
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
      auto_discover: false
      distributed_mode: false
    ollama:
      enabled: true
      base_url: http://192.168.178.126:11434
      additional_servers:
      # gpu_server is same as base_url, so only add localhost as fallback
      - name: localhost
        url: http://localhost:11434
        priority: SECONDARY
      auto_discover: false
      distributed_mode: true
      timeout: 120
      max_retries: 2
      cache_enabled: true
      auto_detect_models: true
      # Модели выбираются автоматически IntelligentModelRouter
      # на основе возможностей модели и метрик производительности
      # Используем llama3.2:3b - доступна на обоих серверах
      default_model: llama3.2:3b
rag:
  enabled: true
  vector_store:
    type: faiss
    dimension: 384
    index_path: vector_store/index.faiss
    metadata_path: vector_store/metadata.pkl
  embeddings:
    model: sentence-transformers/all-MiniLM-L6-v2
    device: cpu
    batch_size: 32
    cache_dir: embeddings_cache
  search:
    top_k: 10
    use_bm25: true
    use_reranking: true
    reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
    query_expansion: true
    multi_query: true
context:
  max_tokens: 8000
  hierarchical: true
  query_expansion: true
  multi_query: true
orchestrator:
  enabled: true
  max_parallel_tasks: 10
  task_timeout: 3600
  auto_recovery: true
  planning:
    strategy: llm
    use_memory: true
    max_depth: 5
  scaling:
    auto_scale: true
    min_parallel_tasks: 3
    max_parallel_tasks: 20
    gpu_tasks_multiplier: 3
agents:
  reflection:
    enabled: true
    max_retries: 2
    min_quality_threshold: 60.0
  code_writer:
    enabled: true
    temperature: 0.7
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 70.0
    use_thinking_mode: false
  react:
    enabled: true
    temperature: 0.5
    max_iterations: 20
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 60.0
    use_thinking_mode: true
  research:
    enabled: true
    temperature: 0.3
    max_iterations: 15
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 50.0
    use_thinking_mode: true
  data_analysis:
    enabled: true
    temperature: 0.2
    max_iterations: 25
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 65.0
    use_thinking_mode: true
  workflow:
    enabled: true
    temperature: 0.3
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
    use_thinking_mode: false
  integration:
    enabled: true
    temperature: 0.4
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
    use_thinking_mode: false
  monitoring:
    enabled: true
    temperature: 0.2
    max_iterations: 10
    reflection:
      enabled: false
    use_thinking_mode: false
memory:
  enabled: true
  storage_path: memory/memories.db
  max_memories: 10000
  similarity_threshold: 0.7
  auto_cleanup: true
  cleanup_interval_days: 30
api:
  host: 0.0.0.0
  port: 8000
  reload: true
  workers: 1
  cors:
    enabled: true
    origins:
    - http://localhost:3000
    - http://localhost:5173
    - http://127.0.0.1:3000
    - http://127.0.0.1:5173
    - http://localhost:8000
    - http://localhost:8001
    - http://localhost:1420
    - http://localhost:1421
    - http://127.0.0.1:1420
    - http://127.0.0.1:1421
  websocket:
    enabled: true
    ping_interval: 20
    ping_timeout: 10
logging:
  level: DEBUG
  format: json
  file: logs/aillm.log
  max_size_mb: 100
  backup_count: 5
performance:
  cache:
    llm_responses: true
    embeddings: true
    search_results: true
  parallel:
    enabled: true
    max_workers: 8
    max_concurrent_requests: 15
  gpu:
    enabled: true
    multi_gpu: true
    devices:
    - cuda:0
    memory_per_gpu_gb: 24
    load_balancing: round_robin
    ollama_gpu_layers: -1
    ollama_num_parallel: 4
    device: cpu
tools:
  enabled: true
  categories:
    file: true
    shell: true
    git: true
    web: true
    database: true
    api: true
    browser: false  # Requires: pip install playwright && playwright install chromium
    document: true  # Requires: pip install pypdf python-docx pandas openpyxl python-pptx
  safety:
    enabled: true
    sandbox: false
    allowed_commands: []
    blocked_patterns:
    - rm -rf /
    - 'format c:'
    - del /f /s /q
multimodal:
  enabled: true
  image:
    enabled: true
    ocr: true
    ocr_engine: tesseract
    max_size_mb: 10
  audio:
    enabled: true
    transcription: true
    model: base
  video:
    enabled: true
    max_duration_seconds: 300
    extract_frames: true
automl:
  enabled: true
  frameworks:
  - sklearn
  - xgboost
  - lightgbm
  - torch
  optimization:
    enabled: true
    framework: optuna
    n_trials: 100
    timeout: 3600
  explainability:
    enabled: true
    methods:
    - shap
    - lime
data:
  processing:
    enabled: true
    frameworks:
    - pandas
    - dask
    - numpy
  etl:
    enabled: true
    streaming: true
  analytics:
    enabled: true
    auto_eda: true
    visualization: true
workflow:
  enabled: true
  visual_editor: true
  storage_path: workflows/
  templates_path: workflows/templates/
frontend:
  enabled: true
  tauri:
    enabled: true
    window_title: AILLM
    window_size:
    - 1200
    - 800
    min_size:
    - 800
    - 600
