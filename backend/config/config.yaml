llm:
  default_provider: ollama
  providers:
    openai:
      enabled: true
      default_model: gpt-4-turbo-preview
      base_url: https://api.openai.com/v1
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
    anthropic:
      enabled: true
      default_model: claude-3-opus-20240229
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
    ollama:
      enabled: true
      # Основной URL - localhost как fallback
      base_url: http://localhost:11434
      
      # Дополнительные серверы для распределённой работы
      # Система проверяет ВСЕ серверы и выбирает лучший
      additional_servers:
        - name: gpu_server              # Удалённый GPU сервер с RTX 3090
          url: http://192.168.178.126:11434
          priority: PRIMARY             # Высший приоритет для мощных моделей
        - name: localhost               # Локальный Ollama
          url: http://localhost:11434
          priority: SECONDARY           # Второй приоритет - быстрые модели
      
      # Авто-обнаружение в локальной сети
      auto_discover: false              # Отключено для скорости
      
      # Распределённый режим: маршрутизация между серверами
      distributed_mode: true
      
      # Таймаут для запросов (секунды)
      timeout: 120
      max_retries: 2
      cache_enabled: true
      auto_detect_models: true
      recommended_models:
        code:
        - qwen2.5-coder:1.5b   # Быстрая для слабых машин
        - stable-code:latest   # Альтернатива
        - qwen2.5-coder:7b
        - qwen2.5-coder:14b
        - deepseek-coder-v2:16b
        chat:
        - gemma3:12b
        - gemma3:4b
        - qwen2.5:14b
        - llama3.3:70b
        analysis:
        - gemma3:12b
        - qwen2.5:14b
        - llama3.3:70b
        # Для веб-поиска и исследований (research mode)
        research:
        - qwen2.5:7b          # Отличный для суммаризации
        - gemma3:4b           # Хороший русский, следует инструкциям
        - qwen2.5:14b         # Мощный анализ
        - gemma3:12b
        - llama3.1:8b         # Универсальный
        reasoning:
        - qwen3:14b
        - qwen3:8b
        - deepseek-r1:14b
        - llama3.3:70b
        fast:
        - gemma3:1b          # Самая быстрая для чата
        - qwen2.5-coder:1.5b # Быстрая для кода
        - stable-code:latest # Альтернатива для кода
        - qwen2.5:1.5b
        - llama3.2:1b
      default_model: gemma3
rag:
  enabled: true
  vector_store:
    type: faiss
    dimension: 384
    index_path: vector_store/index.faiss
    metadata_path: vector_store/metadata.pkl
  embeddings:
    model: sentence-transformers/all-MiniLM-L6-v2
    device: cpu
    batch_size: 32
    cache_dir: embeddings_cache
  search:
    top_k: 10
    use_bm25: true
    use_reranking: true
    reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
    query_expansion: true
    multi_query: true
context:
  max_tokens: 8000
  hierarchical: true
  query_expansion: true
  multi_query: true
orchestrator:
  enabled: true
  max_parallel_tasks: 10  # Увеличено для multi-GPU
  task_timeout: 3600
  auto_recovery: true
  planning:
    strategy: llm
    use_memory: true
    max_depth: 5
  # Масштабирование под ресурсы
  scaling:
    auto_scale: true  # Автоматическое масштабирование под ресурсы
    min_parallel_tasks: 3
    max_parallel_tasks: 20  # Максимум при 3x RTX 3090
    gpu_tasks_multiplier: 3  # Множитель задач на GPU
agents:
  reflection:
    enabled: true
    max_retries: 2
    min_quality_threshold: 60.0
  code_writer:
    enabled: true
    temperature: 0.7
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 70.0
    use_thinking_mode: false
  react:
    enabled: true
    temperature: 0.5
    max_iterations: 20
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 60.0
    use_thinking_mode: true
  research:
    enabled: true
    temperature: 0.3
    max_iterations: 15
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 50.0
    use_thinking_mode: true
  data_analysis:
    enabled: true
    temperature: 0.2
    max_iterations: 25
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 65.0
    use_thinking_mode: true
  workflow:
    enabled: true
    temperature: 0.3
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
    use_thinking_mode: false
  integration:
    enabled: true
    temperature: 0.4
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
    use_thinking_mode: false
  monitoring:
    enabled: true
    temperature: 0.2
    max_iterations: 10
    reflection:
      enabled: false
    use_thinking_mode: false
memory:
  enabled: true
  storage_path: memory/memories.db
  max_memories: 10000
  similarity_threshold: 0.7
  auto_cleanup: true
  cleanup_interval_days: 30
api:
  host: 0.0.0.0
  port: 8000
  reload: true
  workers: 1
  cors:
    enabled: true
    origins:
    - http://localhost:3000
    - http://localhost:5173
    - http://127.0.0.1:3000
    - http://127.0.0.1:5173
    - http://localhost:8000
    - http://localhost:8001
    - http://localhost:1420
    - http://localhost:1421
    - http://127.0.0.1:1420
    - http://127.0.0.1:1421
  websocket:
    enabled: true
    ping_interval: 20
    ping_timeout: 10
logging:
  level: INFO
  format: json
  file: logs/aillm.log
  max_size_mb: 100
  backup_count: 5
performance:
  cache:
    llm_responses: true
    embeddings: true
    search_results: true
  parallel:
    enabled: true
    max_workers: 8  # Увеличено для мощного CPU (12900K)
    max_concurrent_requests: 15  # Параллельные LLM запросы
  gpu:
    enabled: true
    multi_gpu: true  # Поддержка нескольких GPU
    devices:
      - cuda:0  # RTX 3090 #1
      # - cuda:1  # RTX 3090 #2 (раскомментировать после добавления)
      # - cuda:2  # RTX 3090 #3 (раскомментировать после добавления)
    memory_per_gpu_gb: 24  # RTX 3090 = 24GB VRAM
    load_balancing: round_robin  # round_robin, least_loaded, model_affinity
    # Настройки для Ollama multi-GPU
    ollama_gpu_layers: -1  # -1 = все слои на GPU
    ollama_num_parallel: 4  # Параллельные запросы на GPU
tools:
  enabled: true
  categories:
    file: true
    shell: true
    git: true
    web: true
    database: true
    api: true
  safety:
    enabled: true
    sandbox: false
    allowed_commands: []
    blocked_patterns:
    - rm -rf /
    - 'format c:'
    - del /f /s /q
multimodal:
  enabled: true
  image:
    enabled: true
    ocr: true
    ocr_engine: tesseract
    max_size_mb: 10
  audio:
    enabled: true
    transcription: true
    model: base
  video:
    enabled: true
    max_duration_seconds: 300
    extract_frames: true
automl:
  enabled: true
  frameworks:
  - sklearn
  - xgboost
  - lightgbm
  - torch
  optimization:
    enabled: true
    framework: optuna
    n_trials: 100
    timeout: 3600
  explainability:
    enabled: true
    methods:
    - shap
    - lime
data:
  processing:
    enabled: true
    frameworks:
    - pandas
    - dask
    - numpy
  etl:
    enabled: true
    streaming: true
  analytics:
    enabled: true
    auto_eda: true
    visualization: true
workflow:
  enabled: true
  visual_editor: true
  storage_path: workflows/
  templates_path: workflows/templates/
frontend:
  enabled: true
  tauri:
    enabled: true
    window_title: AILLM
    window_size:
    - 1200
    - 800
    min_size:
    - 800
    - 600
