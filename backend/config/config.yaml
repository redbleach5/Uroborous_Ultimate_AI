llm:
  default_provider: ollama
  providers:
    openai:
      enabled: true
      default_model: gpt-4-turbo-preview
      base_url: https://api.openai.com/v1
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
      recommended_models: null
    anthropic:
      enabled: true
      default_model: claude-3-opus-20240229
      base_url: null
      timeout: 60
      max_retries: 3
      cache_enabled: true
      auto_detect_models: false
      recommended_models: null
    ollama:
      enabled: true
      default_model: llama2
      base_url: http://localhost:11434
      timeout: 300
      max_retries: 2
      cache_enabled: true
      auto_detect_models: true
      recommended_models:
        code:
        - "codellama"
        - "deepseek-coder"
        - "mistral"
        chat:
        - "llama2"
        - "mistral"
        - "neural-chat"
        analysis:
        - "llama2"
        - "mistral"
rag:
  enabled: true
  vector_store:
    type: faiss
    dimension: 384
    index_path: vector_store/index.faiss
    metadata_path: vector_store/metadata.pkl
  embeddings:
    model: sentence-transformers/all-MiniLM-L6-v2
    device: cpu
    batch_size: 32
    cache_dir: embeddings_cache
  search:
    top_k: 10
    use_bm25: true
    use_reranking: true
    reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
    query_expansion: true
    multi_query: true
context:
  max_tokens: 8000
  hierarchical: true
  query_expansion: true
  multi_query: true
orchestrator:
  enabled: true
  max_parallel_tasks: 5
  task_timeout: 3600
  auto_recovery: true
  planning:
    strategy: llm
    use_memory: true
    max_depth: 5
agents:
  # Global reflection settings (can be overridden per agent)
  reflection:
    enabled: true
    max_retries: 2
    min_quality_threshold: 60.0
  
  code_writer:
    enabled: true
    default_model: null
    temperature: 0.7
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 70.0  # Higher threshold for code
  react:
    enabled: true
    default_model: null
    temperature: 0.5
    max_iterations: 20
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 60.0
  research:
    enabled: true
    default_model: null
    temperature: 0.3
    max_iterations: 15
    reflection:
      enabled: true
      max_retries: 1  # Less retries for research
      min_quality_threshold: 50.0
  data_analysis:
    enabled: true
    default_model: null
    temperature: 0.2
    max_iterations: 25
    reflection:
      enabled: true
      max_retries: 2
      min_quality_threshold: 65.0
  workflow:
    enabled: true
    default_model: null
    temperature: 0.3
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
  integration:
    enabled: true
    default_model: null
    temperature: 0.4
    max_iterations: 10
    reflection:
      enabled: true
      max_retries: 1
      min_quality_threshold: 60.0
  monitoring:
    enabled: true
    default_model: null
    temperature: 0.2
    max_iterations: 10
    reflection:
      enabled: false  # Monitoring doesn't need reflection
memory:
  enabled: true
  storage_path: memory/memories.db
  max_memories: 10000
  similarity_threshold: 0.7
  auto_cleanup: true
  cleanup_interval_days: 30
api:
  host: 192.168.178.126
  port: 8000
  reload: true
  workers: 1
  cors:
    enabled: true
    origins:
    - "*"
  websocket:
    enabled: true
    ping_interval: 20
    ping_timeout: 10
logging:
  level: INFO
  format: json
  file: logs/aillm.log
  max_size_mb: 100
  backup_count: 5
performance:
  cache:
    llm_responses: true
    embeddings: true
    search_results: true
  parallel:
    enabled: true
    max_workers: 4
  gpu:
    enabled: false
    device: cuda:0
tools:
  enabled: true
  categories:
    file: true
    shell: true
    git: true
    web: true
    database: true
    api: true
  safety:
    enabled: true
    sandbox: false
    allowed_commands: []
    blocked_patterns:
    - "rm -rf /"
    - "format c:"
    - "del /f /s /q"
multimodal:
  enabled: true
  image:
    enabled: true
    ocr: true
    ocr_engine: tesseract
    max_size_mb: 10
  audio:
    enabled: true
    transcription: true
    model: whisper-base
  video:
    enabled: true
    max_duration_seconds: 300
    extract_frames: true
automl:
  enabled: true
  frameworks:
  - "sklearn"
  - "xgboost"
  - "lightgbm"
  - "torch"
  optimization:
    enabled: true
    framework: optuna
    n_trials: 100
    timeout: 3600
  explainability:
    enabled: true
    methods:
    - "shap"
    - "lime"
data:
  processing:
    enabled: true
    frameworks:
    - "pandas"
    - "dask"
    - "numpy"
  etl:
    enabled: true
    streaming: true
  analytics:
    enabled: true
    auto_eda: true
    visualization: true
workflow:
  enabled: true
  visual_editor: true
  storage_path: workflows/
  templates_path: workflows/templates/
frontend:
  enabled: true
  tauri:
    enabled: true
    window_title: AILLM
    window_size:
    - 1200
    - 800
    min_size:
    - 800
    - 600
