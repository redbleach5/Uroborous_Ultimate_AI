# Приоритизация Ollama в проекте

## Обзор

Проект оптимизирован для работы с Ollama как приоритетным провайдером локальных моделей. Все компоненты системы настроены на использование Ollama в первую очередь.

## Основные изменения

### 1. Приоритизация инициализации

**LLMProviderManager** (`backend/llm/providers.py`):
- Ollama инициализируется **первым** среди всех провайдеров
- При выборе провайдера Ollama всегда проверяется в первую очередь
- Если Ollama доступен, он используется как default provider

### 2. Умный выбор моделей Ollama

**OllamaProvider** (`backend/llm/ollama_provider.py`):
- Автоматический выбор лучшей модели на основе типа задачи:
  - **code**: codellama, deepseek-coder, mistral
  - **chat**: llama2, mistral, neural-chat
  - **analysis**: llama2, mistral
- Метод `_select_best_model()` выбирает оптимальную модель из доступных
- Использует `recommended_models` из конфигурации

### 3. Реализация thinking mode для Ollama

Ollama поддерживает thinking mode двумя способами:

#### Нативная поддержка (для поддерживающих моделей)

Некоторые модели Ollama имеют встроенную поддержку thinking mode:
- **Llama 3.3** и новее - нативная поддержка thinking
- **Qwen 2.5** - поддерживает reasoning mode
- **DeepSeek** модели - встроенная поддержка thinking

Для этих моделей:
- Используется нативный API параметр `thinking` (если поддерживается версией Ollama)
- Минимальные инструкции в промптах
- Thinking traces извлекаются из структурированного ответа API

#### Эмуляция (для моделей без нативной поддержки)

Для моделей без нативной поддержки thinking mode:
- Метод `_enhance_prompt_for_thinking()` добавляет расширенные инструкции для глубокого мышления
- Системные промпты обогащаются детальными инструкциями по step-by-step reasoning
- Thinking traces извлекаются из текста ответа (поиск маркеров reasoning)

**Автоматическое определение:**
- Система автоматически определяет, поддерживает ли модель нативный thinking mode
- Использует нативную поддержку если доступна, иначе переключается на эмуляцию
- Логирует тип используемого thinking mode для отладки

### 4. Автоматический выбор Ollama в агентах

**BaseAgent** (`backend/agents/base.py`):
- Если провайдер не указан явно, автоматически выбирается Ollama
- Автоматическое определение типа задачи для умного выбора модели:
  - `code_writer` → task_type="code"
  - `react` → task_type="reasoning"
  - `research` → task_type="analysis"
  - `data_analysis` → task_type="analysis"
- Передача `task_type` в LLM провайдер для оптимального выбора модели

### 5. Приоритизация в TaskRouter

**TaskRouter** (`backend/core/task_router.py`):
- Ollama всегда выбирается в первую очередь, если доступен
- Fallback на другие провайдеры только если Ollama недоступен
- Логирование выбора Ollama для отладки

### 6. Конфигурация по умолчанию

**Config** (`backend/config.py`):
- `default_provider: "ollama"` в адаптивных дефолтах
- Агенты настроены с `default_model: None` для автоматического выбора из Ollama
- Рекомендуемые модели для разных типов задач

## Использование

### Автоматический выбор модели

Если `default_model: null` в конфигурации агента, система автоматически выберет лучшую Ollama модель на основе:
1. Типа задачи (code, chat, analysis, reasoning)
2. Доступных моделей в Ollama
3. Рекомендуемых моделей из конфигурации

### Явное указание модели

```yaml
agents:
  code_writer:
    default_model: "codellama"  # Явно указать модель
```

### Явное указание провайдера

В коде агента можно явно указать провайдер:

```python
# Использовать Ollama явно
response = await self._get_llm_response(
    messages=messages,
    provider="ollama"
)

# Использовать другой провайдер (fallback)
response = await self._get_llm_response(
    messages=messages,
    provider="openai"
)
```

## Рекомендуемые модели Ollama

### Для генерации кода
- `codellama` - специализированная модель для кода
- `deepseek-coder` - мощная модель для кода
- `mistral` - универсальная модель с хорошей поддержкой кода

### Для чата и общения
- `llama2` - базовая модель для чата
- `mistral` - быстрая и качественная модель
- `neural-chat` - оптимизирована для диалогов

### Для анализа и исследований
- `llama2` - надежная модель для анализа
- `mistral` - хороший баланс скорости и качества

### Для сложных задач (70B+ параметров)
- `llama2:70b` - мощная модель для сложных задач
- `codellama:70b` - для сложной генерации кода
- `mistral:70b` - универсальная мощная модель
- `llama3:70b` - новейшая модель
- `deepseek-coder:67b` - для сложного кода
- `qwen2.5:72b` - мощная мультиязычная модель
- `mixtral:8x7b` - MoE модель с высокой производительностью

## Преимущества приоритизации Ollama

1. **Локальная работа**: Все данные остаются локально, нет отправки в облако
2. **Бесплатность**: Нет затрат на API вызовы
3. **Приватность**: Полная конфиденциальность данных
4. **Скорость**: Нет задержек сети для локальных моделей
5. **Гибкость**: Можно использовать любые модели Ollama
6. **Масштабируемость**: Поддержка 30+ моделей с оптимизированным connection pooling

## Fallback логика

Если Ollama недоступен:
1. Система автоматически переключается на другие доступные провайдеры
2. Приоритет: OpenAI → Anthropic → другие
3. Логируется предупреждение о недоступности Ollama

## Оптимизации для Ollama

1. **Connection Pooling**: 50 keepalive connections, 100 max connections
2. **HTTP/2**: Используется для лучшей производительности (с fallback на HTTP/1.1)
3. **AdvancedCache**: Многоуровневое кэширование (memory + disk + Redis)
4. **Умный выбор моделей**: Автоматический выбор на основе типа задачи
5. **Эмуляция thinking mode**: Оптимизированные промпты для глубокого мышления

## Мониторинг

Логирование включает:
- Выбор Ollama как приоритетного провайдера
- Выбор конкретной модели на основе типа задачи
- Использование thinking mode эмуляции
- Fallback на другие провайдеры (если Ollama недоступен)

## Рекомендации

1. **Установите Ollama**: Убедитесь, что Ollama запущен на `http://localhost:11434`
2. **Загрузите модели**: Установите рекомендуемые модели для ваших задач
3. **Настройте конфигурацию**: Обновите `recommended_models` под ваши модели
4. **Мониторьте логи**: Следите за выбором моделей и производительностью

## Примеры конфигурации

### Минимальная конфигурация
```yaml
llm:
  default_provider: ollama
  providers:
    ollama:
      enabled: true
      default_model: llama2
      auto_detect_models: true
```

### Расширенная конфигурация
```yaml
llm:
  default_provider: ollama
  providers:
    ollama:
      enabled: true
      default_model: llama2
      auto_detect_models: true
      recommended_models:
        code: ["codellama", "deepseek-coder"]
        chat: ["llama2", "mistral"]
        analysis: ["llama2", "mistral"]
```

### С кэшированием
```yaml
llm:
  default_provider: ollama
  providers:
    ollama:
      enabled: true
      cache_enabled: true
      cache:
        memory_size: 2000
        disk_cache_dir: "cache/ollama"
        ttl: 7200
```

